{
	"name": "synapse_structured_streaming",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "strmsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4c4f8a28-6789-4239-a37c-3f961abce018"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/P3-fasthackpt3-RG/providers/Microsoft.Synapse/workspaces/fasthack-synapse-pt3/bigDataPools/strmsparkpool",
				"name": "strmsparkpool",
				"type": "Spark",
				"endpoint": "https://fasthack-synapse-pt3.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/strmsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Getting the datalake name from key vault\r\n",
					"- "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"datalake_name = mssparkutils.credentials.getSecretWithLS(\"LS_KeyVault\", \"datalakeName\")\r\n",
					"print(datalake_name)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Read the secrets from the key vault: \r\n",
					"- In the KeyVault, the eventhub connection string is stored with the secret name \"eh-conn-str\"\r\n",
					"- We have already created the linked service to the key vault. The linked service name: LS_AKV"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"eh_conn_str = mssparkutils.credentials.getSecretWithLS(\"LS_KeyVault\", \"eh-conn-str\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Variable Delcaration\r\n",
					"- variable for the delta lake path where we are going to save the data.\r\n",
					"- variable path for the checkfile location.\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"table_delta_file_location = f\"abfss://curated@{datalake_name}.dfs.core.windows.net/streaming/spark_structured_streaming_using_synapse/structured_streaming_eh\"\r\n",
					"checkpointLocation = f\"abfss://curated@{datalake_name}.dfs.core.windows.net/streaming/spark_structured_streaming_using_synapse/_checkcpoint/structured_streaming_eh\""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"connectionString = eh_conn_str\r\n",
					"ehConf = {\r\n",
					"  'eventhubs.connectionString' : sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\r\n",
					"}\r\n",
					"def write2table(df2, epoch_id):\r\n",
					"    df2.write.format(\"delta\").mode(\"append\").save(f\"abfss://curated@{datalake_name}.dfs.core.windows.net/streaming/spark_structured_streaming_using_synapse/structured_streaming_eh\")\r\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## read stream data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = spark \\\r\n",
					"    .readStream \\\r\n",
					"    .format(\"eventhubs\") \\\r\n",
					"    .options(**ehConf) \\\r\n",
					"  .load()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## transformation"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df1 = df.withColumn(\"body\", df[\"body\"].cast(\"string\"))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df1.cre"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## write down the stream data into the delta table."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df1.writeStream \\\r\n",
					"    .outputMode(\"update\") \\\r\n",
					"    .trigger(processingTime='5 seconds') \\\r\n",
					"    .option(\"checkpointLocation\",checkpointLocation) \\\r\n",
					"    .foreachBatch(write2table) \\\r\n",
					"    .start() \\\r\n",
					"    .awaitTermination()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## read the data. \r\n",
					"- We need to stop the previous execution and run it here."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = (spark.read.format(\"delta\").load(path=f\"abfss://curated@{datalake_name}.dfs.core.windows.net/streaming/spark_structured_streaming_using_synapse/structured_streaming_eh\"))\r\n",
					"df.createOrReplaceTempView(\"stream_data\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Analyze the data\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"select * from stream_data order by enqueuedTime desc"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Create the delta table.\r\n",
					"- The query is needed to be executed only once. This is just to create the delta table.\r\n",
					"- We can run this once the streaming has started writing the data into delta lake physical location.\r\n",
					"- you need to be the storage blob data contributor to oth the sotrage account. \r\n",
					"1. synapse internal storage account\r\n",
					"2. data lake storage account"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					}
				},
				"source": [
					"  %%sql\r\n",
					"\r\n",
					"--   CREATE DATABASE IF NOT EXISTS STREAMING;\r\n",
					"  "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# df.write.format(\"delta\").mode(\"append\").save(table_delta_file_location)\r\n",
					"# sqltext = f\"CREATE TABLE IF NOT EXISTS streaming.stream_data USING DELTA LOCATION '{table_delta_file_location}'\"\r\n",
					"# print(sqltext)\r\n",
					"# spark.sql(sqltext)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}